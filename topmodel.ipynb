{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced NLP Workbook for Recruitment Chatbot with Advanced Feature Engineering, State Machine, Sentiment Analysis, Personalization and Graph Database Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install rasa rasa-sdk scikit-learn vaderSentiment matplotlib seaborn tqdm scipy wordcloud shap textstat transitions neo4j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Importing Required Libraries\n",
    "\n",
    "We import necessary Python libraries for data manipulation, feature extraction, modeling, evaluation, and visualization.\n",
    "\n",
    "- **Numpy and Pandas** for data manipulation.\n",
    "- **Scikit-learn** for model building, feature extraction, and evaluation.\n",
    "- **VaderSentiment** for sentiment analysis.\n",
    "- **Matplotlib and Seaborn** for data visualization.\n",
    "- **Tqdm** for progress bars to monitor loops and training processes.\n",
    "- **Transitions** for implementing state machines to manage the conversation flow.\n",
    "- **Neo4j** for using graph databases for a flexible conversation flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import PolynomialFeatures, MinMaxScaler\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "from tqdm import tqdm\n",
    "import shap\n",
    "from textstat import flesch_reading_ease\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from transitions import Machine  # State machine for managing question flow\n",
    "from neo4j import GraphDatabase  # Graph database for flexible conversation flow\n",
    "import json  # For storing rule-based logic in a JSON format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Loading, Merging, and Integrating New Datasets\n",
    "\n",
    "In this step, we merge various data sources (`sample_skills.csv`, `sample_job_summary.csv`, and `gd_rev_preprocessed.csv`) to form a unified dataset for further analysis. This helps provide a complete understanding of the job descriptions.\n",
    "\n",
    "- **Data Sources**: Skills, job summaries, and interview Q&A.\n",
    "- **Purpose**: To enrich the dataset with all possible information to produce insightful NLP analysis.\n",
    "- **Merging Strategy**: Merge on `job_title` to ensure that all related information is brought together.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "gsearch_jobs = pd.read_csv('data/gsearch_jobs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 53743 entries, 0 to 53742\n",
      "Data columns (total 27 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   Unnamed: 0           53743 non-null  int64  \n",
      " 1   index                53743 non-null  int64  \n",
      " 2   title                53743 non-null  object \n",
      " 3   company_name         53743 non-null  object \n",
      " 4   location             53706 non-null  object \n",
      " 5   via                  53734 non-null  object \n",
      " 6   description          53743 non-null  object \n",
      " 7   extensions           53743 non-null  object \n",
      " 8   job_id               53743 non-null  object \n",
      " 9   thumbnail            33792 non-null  object \n",
      " 10  posted_at            53708 non-null  object \n",
      " 11  schedule_type        53515 non-null  object \n",
      " 12  work_from_home       25462 non-null  object \n",
      " 13  salary               9199 non-null   object \n",
      " 14  search_term          53743 non-null  object \n",
      " 15  date_time            53743 non-null  object \n",
      " 16  search_location      53743 non-null  object \n",
      " 17  commute_time         0 non-null      float64\n",
      " 18  salary_pay           9199 non-null   object \n",
      " 19  salary_rate          9199 non-null   object \n",
      " 20  salary_avg           9199 non-null   float64\n",
      " 21  salary_min           8647 non-null   float64\n",
      " 22  salary_max           8647 non-null   float64\n",
      " 23  salary_hourly        5620 non-null   float64\n",
      " 24  salary_yearly        3546 non-null   float64\n",
      " 25  salary_standardized  9199 non-null   float64\n",
      " 26  description_tokens   53743 non-null  object \n",
      "dtypes: float64(7), int64(2), object(18)\n",
      "memory usage: 11.1+ MB\n"
     ]
    }
   ],
   "source": [
    "gsearch_jobs.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Data Cleaning and Preprocessing\n",
    "\n",
    "We clean text data to remove any unnecessary characters and prepare the dataset for NLP operations. This involves removing punctuation, converting text to lowercase, and combining key textual information into a single column for analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'\\W+', ' ', text)  # Remove non-word characters\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    return text\n",
    "\n",
    "# Apply text cleaning to relevant columns\n",
    "gsearch_jobs['description_clean'] = gsearch_jobs['description'].apply(lambda x: clean_text(str(x)))\n",
    "\n",
    "# Drop rows with missing descriptions\n",
    "gsearch_jobs.dropna(subset=['description_clean'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Advanced Feature Engineering\n",
    "\n",
    "#### Step 5.1: TF-IDF Vectorization\n",
    "\n",
    "We use **TF-IDF Vectorizer** to convert the textual data into numerical feature vectors that the model can process.\n",
    "\n",
    "- **Why TF-IDF**: It captures the importance of words in a document relative to the corpus, making it a powerful feature extraction technique for NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "X = vectorizer.fit_transform(gsearch_jobs['description_clean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5.2: Polynomial Features and Standard Scaling\n",
    "\n",
    "- **Polynomial Features**: Increase the complexity of our features by generating interaction terms, which can improve model performance when relationships between features are non-linear.\n",
    "- **Standard Scaling**: Standardizes the features by removing the mean and scaling to unit variance, which is especially important for linear models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import ComplementNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add polynomial features to increase feature complexity (Reduced degree to prevent memory issues)\n",
    "poly = PolynomialFeatures(degree=1, interaction_only=True, include_bias=False)\n",
    "X_poly = poly.fit_transform(X.toarray())\n",
    "\n",
    "# Min-Max Scaling to keep features non-negative\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X_poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5.3: Dimensionality Reduction\n",
    "\n",
    "We use **Truncated SVD** to reduce the dimensionality of the TF-IDF matrix. This helps reduce computational cost and overfitting while preserving essential information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Truncated SVD to reduce dimensionality\n",
    "svd = TruncatedSVD(n_components=50, random_state=42)  # Reduce dimensions further to avoid overfitting\n",
    "X_reduced = svd.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Step 6: Introducing State Machine for Conversation Flow Management\n",
    "\n",
    "We use a **State Machine** to define the conversation flow for guiding managers through the recruitment question generation process.\n",
    "\n",
    "#### Step 6.1: Defining States and Transitions\n",
    "\n",
    "- **States**: Represent parts of the conversation (e.g., Role Requirements, Company Environment, Compensation & Benefits).\n",
    "- **Transitions**: Define how the flow moves from one state to another based on the manager's response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "company_environment\n"
     ]
    }
   ],
   "source": [
    "from transitions import Machine\n",
    "\n",
    "# Define states for the recruitment conversation flow\n",
    "states = ['role_requirements', 'company_environment', 'compensation_benefits', 'role_nuances', 'final_summary']\n",
    "\n",
    "# Define the state machine model\n",
    "class RecruitmentAssistant:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "\n",
    "# Create an instance of RecruitmentAssistant\n",
    "recruitment_assistant = RecruitmentAssistant(\"Assistant\")\n",
    "\n",
    "# Create a state machine with defined states and transitions\n",
    "machine = Machine(model=recruitment_assistant, states=states, initial='role_requirements')\n",
    "\n",
    "# Define state transitions based on manager inputs\n",
    "machine.add_transition(trigger='ask_company_environment', source='role_requirements', dest='company_environment')\n",
    "machine.add_transition(trigger='ask_compensation', source='company_environment', dest='compensation_benefits')\n",
    "machine.add_transition(trigger='ask_role_nuances', source='compensation_benefits', dest='role_nuances')\n",
    "machine.add_transition(trigger='summarize', source='role_nuances', dest='final_summary')\n",
    "\n",
    "# Example of using the state machine\n",
    "recruitment_assistant.ask_company_environment()\n",
    "print(recruitment_assistant.state)  # Output: company_environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Dynamic Question Generation with Decision Trees\n",
    "\n",
    "We use **Decision Trees** for dynamic questioning, where each node represents a question and each branch represents possible answers leading to different follow-up questions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next question ID: [0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Define sample training data for decision tree - features are hypothetical attributes, target is follow-up question ID\n",
    "X_sample = [[1, 0, 1], [0, 1, 1], [1, 1, 0], [0, 0, 1]]  # Example feature vectors\n",
    "y_sample = [0, 1, 2, 3]  # Follow-up question IDs\n",
    "\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "decision_tree.fit(X_sample, y_sample)\n",
    "\n",
    "# Use decision tree to determine the next question\n",
    "sample_input = [1, 0, 1]\n",
    "next_question = decision_tree.predict([sample_input])\n",
    "print(f\"Next question ID: {next_question}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Introducing Personalization and Rule-Based Logic\n",
    "\n",
    "We use **NLP models** for evaluating sentiment, determining the conversation tone, and dynamically adjusting questions to improve personalization.\n",
    "\n",
    "#### Step 8.1: Sentiment Analysis and Adaptive Questioning\n",
    "\n",
    "We use **Na√Øve Bayes** or **Logistic Regression** models for text classification to evaluate the sentiment of responses and determine the conversation's engagement level.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Negative values in data passed to ComplementNB (input X)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23780\\402728929.py\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtext_clf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mComplementNB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0my_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_reduced\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Randomly generated labels for demonstration, ensure no negative values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mtext_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_reduced\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# Analyze sentiment and adjust follow-up\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\Capstone\\your_envs_directory\\mlenv\\lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    724\u001b[0m         \u001b[0mn_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    725\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_counters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 726\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    727\u001b[0m         \u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_alpha\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    728\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_feature_log_prob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\Capstone\\your_envs_directory\\mlenv\\lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36m_count\u001b[1;34m(self, X, Y)\u001b[0m\n\u001b[0;32m    976\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    977\u001b[0m         \u001b[1;34m\"\"\"Count feature occurrences.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 978\u001b[1;33m         \u001b[0mcheck_non_negative\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ComplementNB (input X)\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    979\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_count_\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    980\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_count_\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\Capstone\\your_envs_directory\\mlenv\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_non_negative\u001b[1;34m(X, whom)\u001b[0m\n\u001b[0;32m   1370\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1371\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mX_min\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1372\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Negative values in data passed to %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mwhom\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1373\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1374\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Negative values in data passed to ComplementNB (input X)"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Train a simple sentiment model (example data)\n",
    "text_clf = ComplementNB()\n",
    "y_labels = np.abs(np.random.choice([0, 1], len(X_reduced)))  # Randomly generated labels for demonstration, ensure no negative values\n",
    "text_clf.fit(X_reduced, y_labels)\n",
    "\n",
    "# Analyze sentiment and adjust follow-up\n",
    "sample_response = \"I would prefer a remote work setting.\"\n",
    "sample_vector = vectorizer.transform([sample_response])\n",
    "sentiment = text_clf.predict(sample_vector)\n",
    "if sentiment == 1:\n",
    "    print(\"Positive sentiment detected, proceeding with follow-up questions about remote tools and flexibility.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 8.2: Rule-Based Logic Stored in JSON\n",
    "\n",
    "To make the decision flow configurable and easier to maintain, we store the conversation rules in a JSON file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define rules for conversation flow in a JSON format\n",
    "rules = {\n",
    "    \"role_requirements\": {\n",
    "        \"next\": \"company_environment\",\n",
    "        \"questions\": [\"What are the must-have skills for this role?\", \"Are there any certifications required?\"]\n",
    "    },\n",
    "    \"company_environment\": {\n",
    "        \"next\": \"compensation_benefits\",\n",
    "        \"questions\": [\"How many people are in the team?\", \"Can you describe the company culture?\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Example usage of JSON-based rules\n",
    "current_state = \"role_requirements\"\n",
    "for question in rules[current_state][\"questions\"]:\n",
    "    print(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Combining Predefined Question Templates and Dynamic Elements\n",
    "\n",
    "We blend **predefined question templates** with dynamically generated content to ensure the conversation is both personalized and comprehensive.\n",
    "\n",
    "- Start with a core set of questions (e.g., role-specific skills).\n",
    "- Adaptively generate follow-up prompts based on previous answers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of predefined question and dynamically generated follow-up\n",
    "core_questions = [\"What are the must-have skills for this role?\"]\n",
    "response = \"This role is temporary.\"\n",
    "\n",
    "# Use response to create a personalized follow-up\n",
    "if \"temporary\" in response.lower():\n",
    "    follow_up = \"Considering that the role is temporary, would you like to discuss the option for contract renewal and team integration procedures?\"\n",
    "    core_questions.append(follow_up)\n",
    "\n",
    "for question in core_questions:\n",
    "    print(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10: Splitting Data for Training and Evaluation\n",
    "\n",
    "We split our dataset into training and testing sets to evaluate our model's performance accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reduced, gsearch_jobs['title'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 11: Model Training\n",
    "\n",
    "We use an **SGDClassifier**, a linear model with stochastic gradient descent learning, which is efficient for large datasets.\n",
    "\n",
    "- **Why SGD**: It works well with high-dimensional data and supports various loss functions suitable for classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train SGD Classifier\n",
    "model = SGDClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Display training progress\n",
    "print(\"Model training complete. Now proceeding to evaluation...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 12: Model Evaluation, Sentiment, Interview Response Analysis, and Explainability\n",
    "\n",
    "#### Step 12.1: Sentiment and Interview Response Analysis\n",
    "\n",
    "We add **sentiment analysis** to understand the overall sentiment behind the 'pros' and 'cons' sections, and the candidate interview responses. This helps gauge candidates' attitudes and alignment with company culture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Sentiment analysis for pros and cons\n",
    "gsearch_jobs['pros_sentiment'] = gsearch_jobs['description_tokens'].apply(lambda x: analyzer.polarity_scores(str(x))['compound'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 12.2: Enhanced Analysis\n",
    "\n",
    "We use various metrics to assess content quality and coverage, ensuring that the model-generated questions align with the hiring requirements and cover essential job aspects.\n",
    "\n",
    "### Step 13: Explainability with SHAP\n",
    "\n",
    "We use **SHAP (SHapley Additive exPlanations)** to explain the output of our model, providing transparency in decision-making and helping us understand which features are most influential in predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a small subset of the training data to fit the SHAP explainer\n",
    "explainer = shap.Explainer(model, X_train[:100])\n",
    "shap_values = explainer(X_test[:10])\n",
    "\n",
    "# Plot summary of the SHAP values\n",
    "shap.summary_plot(shap_values, X_test[:10], feature_names=vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 14: Combining Graph Database for Conversation Flexibility\n",
    "\n",
    "Using a **Graph Database** like Neo4j to store and navigate through your conversation flow provides flexibility to adapt questions based on user interaction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish a connection to Neo4j database\n",
    "driver = GraphDatabase.driver(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "\n",
    "# Define a function to add nodes and relationships\n",
    "def add_question(tx, question, answer_options):\n",
    "    tx.run(\"CREATE (q:Question {text: $question})\", question=question)\n",
    "    for option in answer_options:\n",
    "        tx.run(\"MATCH (q:Question {text: $question}) CREATE (q)-[:HAS_OPTION]->(:Option {text: $option})\", question=question, option=option)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
