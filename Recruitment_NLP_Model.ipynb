{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Recruitment_NLP_Model.ipynb\n",
    "\n",
    "# In this notebook, we will work on developing an NLP model to support our Recruitment Need Analysis Tool. \n",
    "# This model aims to help identify job-specific requirements to ensure a good fit between candidates and roles.\n",
    "# Let's start by understanding our dataset and going step-by-step towards creating a well-performing model.\n",
    "\n",
    "# Step 1: Import Necessary Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Step 2: Load and Prepare Data\n",
    "data_path = '/mnt/data/job_data.csv'  # Replace with actual path to your dataset\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Assuming the dataset has columns ['job_title', 'job_summary', 'skills', 'requirements', 'labels']\n",
    "\n",
    "# Step 3: Text Preprocessing\n",
    "# Combine relevant text fields to create a single column for modeling\n",
    "data['text'] = data['job_title'] + ' ' + data['job_summary'] + ' ' + data['skills'] + ' ' + data['requirements']\n",
    "\n",
    "# Remove any missing values\n",
    "data.dropna(subset=['text', 'labels'], inplace=True)\n",
    "\n",
    "# Step 4: Split Data into Training and Testing\n",
    "X = data['text']\n",
    "y = data['labels']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 5: Vectorization using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Step 6: Train Logistic Regression Model\n",
    "logistic_model = LogisticRegression(max_iter=200)\n",
    "logistic_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Save the Logistic Regression model\n",
    "with open('logistic_model.pkl', 'wb') as model_file:\n",
    "    pickle.dump(logistic_model, model_file)\n",
    "\n",
    "# Step 7: Evaluate Logistic Regression Model\n",
    "y_pred_logistic = logistic_model.predict(X_test_tfidf)\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred_logistic))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_logistic))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_logistic))\n",
    "\n",
    "# Step 8: Train Random Forest Classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Save the Random Forest model\n",
    "with open('random_forest_model.pkl', 'wb') as model_file:\n",
    "    pickle.dump(rf_model, model_file)\n",
    "\n",
    "# Step 9: Evaluate Random Forest Model\n",
    "y_pred_rf = rf_model.predict(X_test_tfidf)\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_rf))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_rf))\n",
    "\n",
    "# Step 10: Conclusion and Next Steps\n",
    "# - We have successfully built and evaluated two models: Logistic Regression and Random Forest.\n",
    "# - Now, we need to choose the best performing model and integrate it into our web application.\n",
    "# - Let's also work on improving the model by adding more data or performing hyperparameter tuning.\n",
    "\n",
    "# Step 11: Loading the Model for Deployment\n",
    "# When integrating the model into the web app, you can use the following code to load it:\n",
    "# with open('logistic_model.pkl', 'rb') as model_file:\n",
    "#     loaded_model = pickle.load(model_file)\n",
    "# predictions = loaded_model.predict(new_data_tfidf)\n",
    "\n",
    "# This notebook serves as the foundation for building and deploying a working Recruitment Need Analysis Tool\n",
    "# that will help employers find the best candidates for the job.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
