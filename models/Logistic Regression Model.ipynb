{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\gerri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gerri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Data Science Lifecycle Pipeline\n",
    "\n",
    "# Import Necessary Libraries\n",
    "import warnings\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "from imblearn.combine import SMOTETomek\n",
    "from tqdm import tqdm\n",
    "from langchain_groq import ChatGroq\n",
    "from gensim.models import Word2Vec\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import json\n",
    "\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# FUTURISTIC PLOT STYLES - LOADED AT THE BEGINNING\n",
    "plt.style.use('dark_background')\n",
    "\n",
    "# Set up LLM with ChatGroq\n",
    "llm = ChatGroq(\n",
    "    model=\"llama3-8b-8192\",\n",
    "    temperature=0, \n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Job Descriptions: 100%|██████████| 334/334 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# Step 1.2: Load Data Files\n",
    "# Loading Data Paths\n",
    "it_job_desc_path = '../data/json/IT Job Desc Annotated Detailed.json'\n",
    "salaries_path = '../data/json/salaries.json'\n",
    "\n",
    "# Step 1.3: Load Data\n",
    "with open(it_job_desc_path, 'r') as file:\n",
    "    it_job_desc_data = json.load(file)\n",
    "\n",
    "# Extract relevant information from JSON with progress tracking\n",
    "records = []\n",
    "for item in tqdm(it_job_desc_data['annotations'], desc=\"Processing Job Descriptions\"):\n",
    "    text = item[0].strip()\n",
    "    entities = item[1]['entities']\n",
    "    if text and text != '\\r':  # Filter out empty or meaningless text\n",
    "        records.append({'text': text, 'entities': entities})\n",
    "\n",
    "# Convert JSON data to DataFrame\n",
    "it_job_desc = pd.DataFrame(records)\n",
    "\n",
    "# Remove job descriptions with no entities\n",
    "it_job_desc = it_job_desc[it_job_desc['entities'].apply(lambda x: len(x) > 0)]\n",
    "\n",
    "# Load salaries data\n",
    "salaries_data = pd.read_json(salaries_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IT Job Description Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 120 entries, 5 to 217\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   text      120 non-null    object\n",
      " 1   entities  120 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 2.8+ KB\n",
      "\n",
      "Salaries Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8805 entries, 0 to 8804\n",
      "Data columns (total 11 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   work_year           8805 non-null   int64 \n",
      " 1   experience_level    8805 non-null   object\n",
      " 2   employment_type     8805 non-null   object\n",
      " 3   job_title           8805 non-null   object\n",
      " 4   salary              8805 non-null   int64 \n",
      " 5   salary_currency     8805 non-null   object\n",
      " 6   salary_in_usd       8805 non-null   int64 \n",
      " 7   employee_residence  8805 non-null   object\n",
      " 8   remote_ratio        8805 non-null   int64 \n",
      " 9   company_location    8805 non-null   object\n",
      " 10  company_size        8805 non-null   object\n",
      "dtypes: int64(4), object(7)\n",
      "memory usage: 756.8+ KB\n"
     ]
    }
   ],
   "source": [
    "# PART 2: Data Cleaning and Exploratory Data Analysis (EDA)\n",
    "# Step 2.1: Data Inspection\n",
    "# Checking for Missing Values and Basic Data Info\n",
    "print(\"IT Job Description Data Info:\")\n",
    "it_job_desc.info()\n",
    "print(\"\\nSalaries Data Info:\")\n",
    "salaries_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "salaries_data[\"job_title\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample Rows from IT Job Descriptions:\n",
      "                                                 text  \\\n",
      "5   Uplimit is seeking a talented Analytics Engine...   \n",
      "7   Collaborate with cross-functional teams to def...   \n",
      "8   Design, develop, and maintain data pipelines, ...   \n",
      "10  Implement data visualization tools to communic...   \n",
      "12  Stay current on industry trends and best pract...   \n",
      "\n",
      "                                             entities  \n",
      "5   [[30, 48, JOB POSITION], [81, 99, JOB POSITION...  \n",
      "7         [[43, 72, IT SKILLS], [77, 106, IT SKILLS]]  \n",
      "8   [[30, 44, IT SKILLS], [46, 59, IT SKILLS], [65...  \n",
      "10                              [[10, 28, IT SKILLS]]  \n",
      "12                              [[54, 68, IT SKILLS]]  \n",
      "\n",
      "Sample Rows from Salaries Data:\n",
      "   work_year experience_level employment_type                       job_title  \\\n",
      "0       2023               EX              FT           Data Science Director   \n",
      "1       2023               EX              FT           Data Science Director   \n",
      "2       2023               MI              FT  Business Intelligence Engineer   \n",
      "3       2023               MI              FT  Business Intelligence Engineer   \n",
      "4       2023               SE              FT       Machine Learning Engineer   \n",
      "\n",
      "   salary salary_currency  salary_in_usd employee_residence  remote_ratio  \\\n",
      "0  212000             USD         212000                 US             0   \n",
      "1  190000             USD         190000                 US             0   \n",
      "2   35000             GBP          43064                 GB             0   \n",
      "3   35000             GBP          43064                 GB             0   \n",
      "4  245700             USD         245700                 US             0   \n",
      "\n",
      "  company_location company_size  \n",
      "0               US            M  \n",
      "1               US            M  \n",
      "2               GB            M  \n",
      "3               GB            M  \n",
      "4               US            M  \n"
     ]
    }
   ],
   "source": [
    "# Step 2.2: Displaying Sample Rows\n",
    "print(\"\\nSample Rows from IT Job Descriptions:\")\n",
    "print(it_job_desc.head())\n",
    "print(\"\\nSample Rows from Salaries Data:\")\n",
    "print(salaries_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2.3: Handling Missing Values\n",
    "# Dropping rows with missing values for simplicity (can be handled differently based on requirements)\n",
    "it_job_desc_cleaned = it_job_desc.dropna()\n",
    "salaries_data_cleaned = salaries_data.dropna()\n",
    "\n",
    "# Explanation: We drop missing values for simplicity; further handling can include imputing or flagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary Statistics for IT Job Descriptions:\n",
      "                                                     text  \\\n",
      "count                                                 120   \n",
      "unique                                                119   \n",
      "top     Bachelor's degree in Computer Science, Enginee...   \n",
      "freq                                                    2   \n",
      "\n",
      "                    entities  \n",
      "count                    120  \n",
      "unique                   116  \n",
      "top     [[0, 71, EDUCATION]]  \n",
      "freq                       2  \n",
      "\n",
      "Summary Statistics for Salaries Data:\n",
      "          work_year experience_level employment_type      job_title  \\\n",
      "count   8805.000000             8805            8805           8805   \n",
      "unique          NaN                4               4            124   \n",
      "top             NaN               SE              FT  Data Engineer   \n",
      "freq            NaN             6336            8762           2062   \n",
      "mean    2022.737422              NaN             NaN            NaN   \n",
      "std        0.542484              NaN             NaN            NaN   \n",
      "min     2020.000000              NaN             NaN            NaN   \n",
      "25%     2023.000000              NaN             NaN            NaN   \n",
      "50%     2023.000000              NaN             NaN            NaN   \n",
      "75%     2023.000000              NaN             NaN            NaN   \n",
      "max     2023.000000              NaN             NaN            NaN   \n",
      "\n",
      "              salary salary_currency  salary_in_usd employee_residence  \\\n",
      "count   8.805000e+03            8805    8805.000000               8805   \n",
      "unique           NaN              22            NaN                 86   \n",
      "top              NaN             USD            NaN                 US   \n",
      "freq             NaN            8006            NaN               7527   \n",
      "mean    1.747287e+05             NaN  149488.265645                NaN   \n",
      "std     4.560690e+05             NaN   64222.105058                NaN   \n",
      "min     1.400000e+04             NaN   15000.000000                NaN   \n",
      "25%     1.055000e+05             NaN  105000.000000                NaN   \n",
      "50%     1.441000e+05             NaN  142200.000000                NaN   \n",
      "75%     1.900000e+05             NaN  185900.000000                NaN   \n",
      "max     3.040000e+07             NaN  615201.000000                NaN   \n",
      "\n",
      "        remote_ratio company_location company_size  \n",
      "count    8805.000000             8805         8805  \n",
      "unique           NaN               74            3  \n",
      "top              NaN               US            M  \n",
      "freq             NaN             7576         7881  \n",
      "mean       38.693924              NaN          NaN  \n",
      "std        48.068060              NaN          NaN  \n",
      "min         0.000000              NaN          NaN  \n",
      "25%         0.000000              NaN          NaN  \n",
      "50%         0.000000              NaN          NaN  \n",
      "75%       100.000000              NaN          NaN  \n",
      "max       100.000000              NaN          NaN  \n"
     ]
    }
   ],
   "source": [
    "# Step 2.4: Summary Statistics\n",
    "print(\"\\nSummary Statistics for IT Job Descriptions:\")\n",
    "print(it_job_desc_cleaned.describe(include='all'))\n",
    "print(\"\\nSummary Statistics for Salaries Data:\")\n",
    "print(salaries_data_cleaned.describe(include='all'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text column contains some repeated content like \"Responsibilities:\" that appears multiple times.\n",
    "The entities column often contains empty lists ([]), indicating that not all job descriptions have annotated entities. This could affect downstream analysis, particularly the extraction of skills and qualifications.\n",
    "\n",
    "There are 8805 entries in the salaries dataset, which is quite large. Some summary statistics like mean and median are already provided.\n",
    "The column salary_in_usd has already been scaled using StandardScaler, resulting in values centered around zero.\n",
    "The majority of jobs are in the United States (company_location shows a top value of 'US'), and the most frequent job title is \"Data Engineer.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample Entities from Job Descriptions:\n",
      "5     [[30, 48, JOB POSITION], [81, 99, JOB POSITION...\n",
      "7           [[43, 72, IT SKILLS], [77, 106, IT SKILLS]]\n",
      "8     [[30, 44, IT SKILLS], [46, 59, IT SKILLS], [65...\n",
      "10                                [[10, 28, IT SKILLS]]\n",
      "12                                [[54, 68, IT SKILLS]]\n",
      "14                                 [[0, 71, EDUCATION]]\n",
      "15           [[19, 30, IT SKILLS], [32, 53, IT SKILLS]]\n",
      "16    [[15, 18, IT LANGUAGES], [20, 23, IT LANGUAGES...\n",
      "17                                [[51, 66, IT SKILLS]]\n",
      "18    [[13, 26, IT SKILLS], [28, 41, IT SKILLS], [71...\n",
      "Name: entities, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# PART 3: Enhance Data Processing - Extracting Skills and Qualifications from Entities\n",
    "# Step 3.1: Analyze Entities to Understand Their Structure\n",
    "print(\"\\nSample Entities from Job Descriptions:\")\n",
    "print(it_job_desc_cleaned['entities'].head(10))\n",
    "\n",
    "# Explanation: This helps us understand the content and structure of the entities to adjust the extraction logic accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Skills and Qualifications: 100%|██████████| 120/120 [00:00<00:00, 13317.36it/s]\n"
     ]
    }
   ],
   "source": [
    "# Step 3.2: Extracting Skills and Qualifications from Entity Labels\n",
    "# Extracting entities into a separate DataFrame for better analysis\n",
    "skills = []\n",
    "qualifications = []\n",
    "other_entities = []\n",
    "\n",
    "for idx, row in tqdm(it_job_desc_cleaned.iterrows(), total=it_job_desc_cleaned.shape[0], desc=\"Extracting Skills and Qualifications\"):\n",
    "    text = row['text']\n",
    "    entities = row['entities']\n",
    "    if entities:  # Only process if entities list is not empty\n",
    "        for entity in entities:\n",
    "            if isinstance(entity, list) and len(entity) == 3:\n",
    "                # Extract content if entity contains three elements: start, end, and label\n",
    "                start_idx, end_idx, label = entity\n",
    "                extracted_text = text[start_idx:end_idx].strip()\n",
    "                label = label.strip().upper()\n",
    "                if label in ['IT SKILLS', 'TECHNOLOGY', 'SOFT SKILLS', 'IT LANGUAGES']:\n",
    "                    skills.append(extracted_text)\n",
    "                elif label in ['QUALIFICATION', 'EDUCATION', 'CERTIFICATION', 'DEGREE']:\n",
    "                    qualifications.append(extracted_text)\n",
    "                else:\n",
    "                    other_entities.append(f\"{label}: {extracted_text}\")\n",
    "\n",
    "# Explanation: Extract actual content using start and end indices to get precise sections of the text, based on labeled entity types.\n",
    "# Additionally, added a condition to ensure only non-empty entities are processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Counter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Step 3.3: Creating DataFrames for Skills, Qualifications, and Other Entities\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m skills_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[43mCounter\u001b[49m(skills)\u001b[38;5;241m.\u001b[39mmost_common(), columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSkill\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      3\u001b[0m qualifications_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(Counter(qualifications)\u001b[38;5;241m.\u001b[39mmost_common(), columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQualification\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      4\u001b[0m other_entities_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(Counter(other_entities)\u001b[38;5;241m.\u001b[39mmost_common(), columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEntity\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Counter' is not defined"
     ]
    }
   ],
   "source": [
    "# Step 3.3: Creating DataFrames for Skills, Qualifications, and Other Entities\n",
    "skills_df = pd.DataFrame(Counter(skills).most_common(), columns=['Skill', 'Frequency'])\n",
    "qualifications_df = pd.DataFrame(Counter(qualifications).most_common(), columns=['Qualification', 'Frequency'])\n",
    "other_entities_df = pd.DataFrame(Counter(other_entities).most_common(), columns=['Entity', 'Frequency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3.4: Displaying Most Common Skills, Qualifications, and Other Entities\n",
    "print(\"\\nMost Common Skills:\")\n",
    "print(skills_df.head())\n",
    "print(\"\\nMost Common Qualifications:\")\n",
    "print(qualifications_df.head())\n",
    "print(\"\\nOther Entities Found:\")\n",
    "print(other_entities_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3.5: Visualizing Most Common Skills\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Frequency', y='Skill', data=skills_df.head(10), palette='viridis')\n",
    "plt.title('Top 10 Most Common Skills')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Skill')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3.6: Visualizing Most Common Qualifications\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Frequency', y='Qualification', data=qualifications_df.head(10), palette='viridis')\n",
    "plt.title('Top 10 Most Common Qualifications')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Qualification')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 4: Preprocessing Data for Analysis\n",
    "# Step 4.1: Preprocessing Data for Analysis using TF-IDF\n",
    "print(\"\\nApplying TF-IDF Vectorization...\")\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "x_tfidf = vectorizer.fit_transform(tqdm(it_job_desc_cleaned['text'], desc=\"TF-IDF Vectorization\"))\n",
    "\n",
    "# Align salaries data and job descriptions based on available data\n",
    "min_samples = min(len(it_job_desc_cleaned), len(salaries_data_cleaned))\n",
    "it_job_desc_filtered = it_job_desc_cleaned.iloc[:min_samples]\n",
    "salaries_data_filtered = salaries_data_cleaned.iloc[:min_samples]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 5: Balancing Data Using SMOTE\n",
    "print(\"\\nApplying SMOTE for Class Imbalance...\")\n",
    "filtered_jobs = salaries_data_filtered['job_title'].value_counts()\n",
    "valid_jobs = filtered_jobs[filtered_jobs > 1].index\n",
    "salaries_data_filtered = salaries_data_filtered[salaries_data_filtered['job_title'].isin(valid_jobs)]\n",
    "x_tfidf_filtered = x_tfidf[:len(salaries_data_filtered)]\n",
    "\n",
    "smote = SMOTE(random_state=42, k_neighbors=1)  # Adjusted k_neighbors to avoid error with small sample sizes\n",
    "x_resampled, y_resampled = smote.fit_resample(x_tfidf_filtered, salaries_data_filtered['job_title'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 6: Model Training and Evaluation\n",
    "# Step 6.1: Splitting Data into Training and Test Sets\n",
    "print(\"\\nSplitting Data into Training and Test Sets...\")\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_resampled, y_resampled, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6.2: Training Logistic Regression Model\n",
    "print(\"\\nTraining Logistic Regression Model...\")\n",
    "logreg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "logreg.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6.3: Evaluating Model Performance\n",
    "print(\"\\nEvaluating Model Performance...\")\n",
    "y_pred = logreg.predict(x_test)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone_compatible",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
