{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Necessary Libraries\n",
    "import warnings\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "from imblearn.combine import SMOTETomek\n",
    "from tqdm import tqdm\n",
    "from langchain_groq import ChatGroq\n",
    "from gensim.models import Word2Vec\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# FUTURISTIC PLOT STYLES - LOADED AT THE BEGINNING\n",
    "plt.style.use('dark_background')\n",
    "\n",
    "# Set up LLM with ChatGroq\n",
    "llm = ChatGroq(\n",
    "    model=\"llama3-8b-8192\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2\n",
    ")\n",
    "\n",
    "# PART 1: Load the Data\n",
    "# Loading datasets\n",
    "salaries_path = '../data/json/salaries.json'\n",
    "salaries_data = pd.read_json(salaries_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "api_key = os.getenv('groq_api_key')\n",
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"llama3-8b-8192\",\n",
    "    api_key=api_key,\n",
    "    temperature=0, \n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Test Failed. Please check your API key and network connection. Got unknown type W\n"
     ]
    }
   ],
   "source": [
    "test_prompt = \"What is the capital of France?\"\n",
    "try:\n",
    "    response = llm.generate(test_prompt)\n",
    "    print(\"API Test Successful, Response:\", response)\n",
    "except Exception as e:\n",
    "    print(\"API Test Failed. Please check your API key and network connection.\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 2: Data Cleaning\n",
    "print(\"\\nCleaning data...\")\n",
    "# Drop duplicates, handle missing values, and perform any necessary preprocessing\n",
    "salaries_data_cleaned = salaries_data.dropna()\n",
    "\n",
    "# Removing outliers based on salary (e.g., removing top/bottom 1% if applicable)\n",
    "q_low = salaries_data_cleaned['salary'].quantile(0.01)\n",
    "q_high = salaries_data_cleaned['salary'].quantile(0.99)\n",
    "salaries_data_cleaned = salaries_data_cleaned[(salaries_data_cleaned['salary'] > q_low) & (salaries_data_cleaned['salary'] < q_high)]\n",
    "\n",
    "# Converting work_year to datetime format\n",
    "salaries_data_cleaned['work_year'] = pd.to_datetime(salaries_data_cleaned['work_year'], format='%Y', errors='coerce')\n",
    "\n",
    "# Cleaning Job Titles\n",
    "print(\"\\nCleaning Job Titles...\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "salaries_data_cleaned['job_title_cleaned'] = salaries_data_cleaned['job_title'].str.lower()\n",
    "salaries_data_cleaned['job_title_cleaned'] = salaries_data_cleaned['job_title_cleaned'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))  # Remove punctuations\n",
    "salaries_data_cleaned['job_title_cleaned'] = salaries_data_cleaned['job_title_cleaned'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split() if word not in stop_words]))  # Lemmatization and stop word removal\n",
    "\n",
    "# Group Rare Job Titles\n",
    "print(\"\\nGrouping Rare Job Titles...\")\n",
    "job_title_counts = salaries_data_cleaned['job_title_cleaned'].value_counts()\n",
    "threshold = 50  # Threshold to group rare job titles\n",
    "salaries_data_cleaned['job_title_grouped'] = salaries_data_cleaned['job_title_cleaned'].apply(\n",
    "    lambda x: x if job_title_counts[x] >= threshold else 'Other'\n",
    ")\n",
    "\n",
    "# Using LLM to Group Similar Job Titles\n",
    "print(\"\\nUsing LLM to Group Similar Job Titles...\")\n",
    "job_titles_to_group = salaries_data_cleaned['job_title_grouped'].unique().tolist()\n",
    "job_title_prompt = f\"Group the following job titles based on similarity: {job_titles_to_group}.\"\n",
    "\n",
    "try:\n",
    "    # The LLM expects a list of dictionaries in a specific format, we'll use a simpler text-based prompt\n",
    "    response = llm.generate([{\"role\": \"user\", \"content\": job_title_prompt}])\n",
    "    job_title_groups = response[0]['choices'][0]['message']['content']\n",
    "    print(\"LLM Suggested Job Title Groups:\", job_title_groups)\n",
    "except Exception as e:\n",
    "    print(\"[ERROR] Could not generate job title groups using LLM.\", e)\n",
    "    job_title_groups = \"No grouping available due to an error.\"\n",
    "\n",
    "\n",
    "# Encoding Categorical Columns for Model Compatibility\n",
    "print(\"\\nEncoding Categorical Features...\")\n",
    "categorical_columns = ['experience_level', 'employment_type', 'job_title_grouped', 'employee_residence', 'company_location', 'company_size']\n",
    "label_encoders = {}\n",
    "for col in tqdm(categorical_columns, desc=\"Encoding categorical features\"):\n",
    "    le = LabelEncoder()\n",
    "    salaries_data_cleaned[col] = le.fit_transform(salaries_data_cleaned[col])\n",
    "    label_encoders[col] = le\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 3: Exploratory Data Analysis (EDA)\n",
    "print(\"\\nPerforming Exploratory Data Analysis...\")\n",
    "# Visualize salary distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "salaries_data_cleaned['salary'].plot(kind='hist', title='Salary Distribution', color='green')\n",
    "plt.xlabel('Salary')\n",
    "plt.show()\n",
    "\n",
    "# Plot job title counts to understand class imbalance (Filtered for Readability)\n",
    "top_n = 20\n",
    "top_job_titles = salaries_data_cleaned['job_title_grouped'].value_counts().nlargest(top_n)\n",
    "plt.figure(figsize=(12, 6))\n",
    "top_job_titles.plot(kind='bar', title='Top Job Titles Counts', color='cyan')\n",
    "plt.xlabel('Job Title')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlation Heatmap (Removing non-numeric columns to avoid errors)\n",
    "plt.figure(figsize=(12, 8))\n",
    "numeric_cols = salaries_data_cleaned.select_dtypes(include=[np.number])\n",
    "sns.heatmap(numeric_cols.corr(), annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()\n",
    "\n",
    "# Box Plot for Salary by Experience Level\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=salaries_data_cleaned, x='experience_level', y='salary', palette='viridis')\n",
    "plt.title('Salary by Experience Level')\n",
    "plt.xlabel('Experience Level')\n",
    "plt.ylabel('Salary')\n",
    "plt.show()\n",
    "\n",
    "# Count Plot for Employment Type\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=salaries_data_cleaned, x='employment_type', palette='magma')\n",
    "plt.title('Count of Employment Types')\n",
    "plt.xlabel('Employment Type')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Pair Plot\n",
    "sns.pairplot(salaries_data_cleaned[['salary', 'remote_ratio', 'company_size']].dropna(), diag_kind='kde')\n",
    "plt.show()\n",
    "\n",
    "# Bar Plot of Top 10 Locations\n",
    "top_locations = salaries_data_cleaned['employee_residence'].value_counts().nlargest(10)\n",
    "plt.figure(figsize=(12, 6))\n",
    "top_locations.plot(kind='bar', title='Top 10 Job Locations', color='teal')\n",
    "plt.xlabel('Location')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Violin Plot for Salary vs. Job Title Grouped\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.violinplot(data=salaries_data_cleaned[salaries_data_cleaned['job_title_grouped'].isin(top_job_titles.index)], x='job_title_grouped', y='salary', palette='cubehelix')\n",
    "plt.title('Salary Distribution by Job Title Grouped')\n",
    "plt.xlabel('Job Title Grouped')\n",
    "plt.ylabel('Salary')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Scatter Plot for Salary vs. Work Year\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(salaries_data_cleaned['work_year'].dt.year, salaries_data_cleaned['salary'], color='orange')\n",
    "plt.title('Salary vs. Work Year')\n",
    "plt.xlabel('Work Year')\n",
    "plt.ylabel('Salary')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Pie Chart for Experience Level Distribution\n",
    "experience_level_counts = salaries_data_cleaned['experience_level'].value_counts()\n",
    "plt.figure(figsize=(8, 8))\n",
    "experience_level_counts.plot(kind='pie', autopct='%1.1f%%', startangle=140, colors=['skyblue', 'lightgreen', 'coral', 'plum'])\n",
    "plt.title('Experience Level Distribution')\n",
    "plt.ylabel('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 4: Train/Test Split (Before Data Balancing to Avoid Data Leakage)\n",
    "print(\"\\nSplitting Data into Training and Test Sets...\")\n",
    "x_train_full, x_test, y_train_full, y_test = train_test_split(\n",
    "    salaries_data_cleaned[['job_title_grouped', 'experience_level', 'employment_type', 'remote_ratio', 'company_size']], salaries_data_cleaned['salary'], test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 5: Feature Engineering\n",
    "print(\"\\nFeature Engineering...\")\n",
    "# Creating Word2Vec embeddings for job titles\n",
    "print(\"\\nTraining Word2Vec model...\")\n",
    "job_titles = [title.split() for title in x_train_full['job_title_grouped'].astype(str)]\n",
    "word2vec_model = Word2Vec(sentences=job_titles, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "def get_word2vec_vector(text, model):\n",
    "    words = text.split()\n",
    "    vector = np.mean([model.wv[word] for word in words if word in model.wv] or [np.zeros(model.vector_size)], axis=0)\n",
    "    return vector\n",
    "\n",
    "x_train_full['job_title_embedding'] = x_train_full['job_title_grouped'].astype(str).apply(lambda x: get_word2vec_vector(x, word2vec_model))\n",
    "x_test['job_title_embedding'] = x_test['job_title_grouped'].astype(str).apply(lambda x: get_word2vec_vector(x, word2vec_model))\n",
    "\n",
    "# Combine all features into a single array\n",
    "x_train = np.hstack([np.vstack(x_train_full['job_title_embedding'].values), x_train_full.drop(columns=['job_title_grouped', 'job_title_embedding']).values])\n",
    "x_test = np.hstack([np.vstack(x_test['job_title_embedding'].values), x_test.drop(columns=['job_title_grouped', 'job_title_embedding']).values])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 6: Data Balancing (Oversampling/Undersampling)\n",
    "print(\"\\nApplying Combined Oversampling and Undersampling for Class Imbalance...\")\n",
    "# Balancing only the training data to avoid data leakage\n",
    "try:\n",
    "    rus = SMOTETomek(random_state=42, sampling_strategy='auto')\n",
    "    x_train_balanced, y_train_balanced = rus.fit_resample(x_train, y_train_full)\n",
    "except ValueError as e:\n",
    "    print(f\"\\n[ERROR] {e}\")\n",
    "    print(\"\\nConsider reducing 'n_neighbors' in SMOTE or use a simpler resampling strategy.\")\n",
    "    # Alternative: Using RandomUnderSampler if SMOTETomek fails\n",
    "    rus_simple = RandomUnderSampler(random_state=42)\n",
    "    x_train_balanced, y_train_balanced = rus_simple.fit_resample(x_train, y_train_full)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 7: Model Training\n",
    "print(\"\\nTraining RandomForest Model...\")\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(x_train_balanced, y_train_balanced)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 8: Model Evaluation\n",
    "print(\"\\nEvaluating Model Performance...\")\n",
    "y_pred = rf.predict(x_test)\n",
    "\n",
    "# Using F1 Score, Precision, Recall, and Confusion Matrix instead of Accuracy\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 9: Optional - Re-label or Combine Classes for Class Imbalance\n",
    "print(\"\\nConsidering Re-labeling or Combining Classes for Improved Balance...\")\n",
    "# Optionally re-label classes or merge similar categories to improve class balance\n",
    "class_counts = pd.Series(y_train_balanced).value_counts()\n",
    "rare_classes = class_counts[class_counts < 10].index\n",
    "y_train_balanced = ['Other' if label in rare_classes else label for label in y_train_balanced]\n",
    "y_test = ['Other' if label in rare_classes else label for label in y_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 11: Optional - Using LLM for Insights and Improvements\n",
    "print(\"\\nUsing LLM for Model and Feature Insights...\")\n",
    "# Requesting LLM analysis on model performance\n",
    "performance_prompt = f\"The F1 Score is {f1}, Precision is {precision}, Recall is {recall}. What hyperparameters should be tuned to improve this RandomForest model?\"\n",
    "try:\n",
    "    llm_performance_feedback = llm.generate([performance_prompt])[0]['choices'][0]['message']['content']\n",
    "    print(\"LLM Analysis Feedback:\", llm_performance_feedback)\n",
    "except TypeError as e:\n",
    "    print(\"[ERROR] Could not generate insights using LLM.\", e)\n",
    "    llm_performance_feedback = \"No feedback available due to an error.\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone_compatible",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
